{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy import optimize\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>y</th>\n",
       "      <th>loan*housing</th>\n",
       "      <th>default*housing</th>\n",
       "      <th>age&gt;60*balance&lt;0</th>\n",
       "      <th>age_group_30-60</th>\n",
       "      <th>age_group_&lt;30</th>\n",
       "      <th>...</th>\n",
       "      <th>pdays_group_1</th>\n",
       "      <th>pdays_group_2</th>\n",
       "      <th>pdays_group_3</th>\n",
       "      <th>pdays_group_4</th>\n",
       "      <th>previous_group_0</th>\n",
       "      <th>previous_group_1</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45206</th>\n",
       "      <td>45206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45207</th>\n",
       "      <td>45207</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45208</th>\n",
       "      <td>45208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45209</th>\n",
       "      <td>45209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45210</th>\n",
       "      <td>45210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45211 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  default  housing  loan  y  loan*housing  default*housing  \\\n",
       "0               0        0        1     0  0             0                0   \n",
       "1               1        0        1     0  0             0                0   \n",
       "2               2        0        1     1  0             1                0   \n",
       "3               3        0        1     0  0             0                0   \n",
       "4               4        0        0     0  0             0                0   \n",
       "...           ...      ...      ...   ... ..           ...              ...   \n",
       "45206       45206        0        0     0  1             0                0   \n",
       "45207       45207        0        0     0  1             0                0   \n",
       "45208       45208        0        0     0  1             0                0   \n",
       "45209       45209        0        0     0  0             0                0   \n",
       "45210       45210        0        0     0  0             0                0   \n",
       "\n",
       "       age>60*balance<0  age_group_30-60  age_group_<30  ...  pdays_group_1  \\\n",
       "0                     0                1              0  ...              0   \n",
       "1                     0                1              0  ...              0   \n",
       "2                     0                1              0  ...              0   \n",
       "3                     0                1              0  ...              0   \n",
       "4                     0                1              0  ...              0   \n",
       "...                 ...              ...            ...  ...            ...   \n",
       "45206                 0                1              0  ...              0   \n",
       "45207                 0                0              0  ...              0   \n",
       "45208                 0                0              0  ...              1   \n",
       "45209                 0                1              0  ...              0   \n",
       "45210                 0                1              0  ...              1   \n",
       "\n",
       "       pdays_group_2  pdays_group_3  pdays_group_4  previous_group_0  \\\n",
       "0                  0              0              1                 1   \n",
       "1                  0              0              1                 1   \n",
       "2                  0              0              1                 1   \n",
       "3                  0              0              1                 1   \n",
       "4                  0              0              1                 1   \n",
       "...              ...            ...            ...               ...   \n",
       "45206              0              0              1                 1   \n",
       "45207              0              0              1                 1   \n",
       "45208              0              0              0                 0   \n",
       "45209              0              0              1                 1   \n",
       "45210              0              0              0                 0   \n",
       "\n",
       "       previous_group_1  poutcome_failure  poutcome_other  poutcome_success  \\\n",
       "0                     0                 0               0                 0   \n",
       "1                     0                 0               0                 0   \n",
       "2                     0                 0               0                 0   \n",
       "3                     0                 0               0                 0   \n",
       "4                     0                 0               0                 0   \n",
       "...                 ...               ...             ...               ...   \n",
       "45206                 0                 0               0                 0   \n",
       "45207                 0                 0               0                 0   \n",
       "45208                 1                 0               0                 1   \n",
       "45209                 0                 0               0                 0   \n",
       "45210                 1                 0               1                 0   \n",
       "\n",
       "       poutcome_unknown  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "...                 ...  \n",
       "45206                 1  \n",
       "45207                 1  \n",
       "45208                 0  \n",
       "45209                 1  \n",
       "45210                 0  \n",
       "\n",
       "[45211 rows x 64 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset\n",
    "df = pd.read_csv('data.csv', sep = ';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping column unnamed 0\n",
    "#creating X and y\n",
    "df=df.drop(df.columns[0],axis=1)\n",
    "marketing_X = df.drop(['y'], axis=1)\n",
    "marketing_y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Class that performs Logistic Regression:\n",
    "    ---------------\n",
    "    Parameters:\n",
    "    X-features\n",
    "    y-target variable\n",
    "    learningRate-0.000001\n",
    "    tolerance-0.0001\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,X,y, learningRate,tolerance, maxIteration=50000):\n",
    "        \n",
    "        \n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.learningRate=learningRate\n",
    "        self.tolerance=tolerance\n",
    "        self.maxIteration=maxIteration\n",
    "        \n",
    "    def normalize(self,X):\n",
    "        \n",
    "        '''function to scale the train data'''\n",
    "        mean=np.mean(X)\n",
    "        std=np.std(X)\n",
    "        X_norm=(X-mean)/std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm,mean,std\n",
    "    \n",
    "    def normalizeTestData(self,X_test,train_mean,train_std):\n",
    "        \n",
    "        '''function to scale the test data'''\n",
    "        X_norm=(X_test-train_mean)/train_std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm\n",
    "        \n",
    "    \n",
    "    def addX0(self,X):\n",
    "        \n",
    "        '''function to add bias to the dataset'''\n",
    "        return np.column_stack([np.ones([X.shape[0],1]),X])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        \n",
    "        '''function for calculating the probability of belonging to a particular class'''\n",
    "        sig=1/(1+np.exp(-z))\n",
    "        return sig\n",
    "    \n",
    "    def costFunction(self,X,y):\n",
    "        \n",
    "        '''function for returning the cost of the model'''\n",
    "        \n",
    "#         #approach 1\n",
    "        \n",
    "        pred_=np.log(np.ones(X.shape[0])+np.exp(X.dot(self.w))) - X.dot(self.w).dot(y) #negative log-likelihood\n",
    "        cost=pred_.sum()\n",
    "        \n",
    "        #approach 2\n",
    "#         sig=self.sigmoid(X.dot(self.w))\n",
    "#         pred_= y * np.log(sig) + (1-y) * np.log(1-sig)\n",
    "#         cost= pred_.sum()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def gradient(self,X,y):\n",
    "        \n",
    "        '''function for calculationg the gradient'''\n",
    "        \n",
    "        sig=self.sigmoid(X.dot(self.w))\n",
    "        grad=(sig-y).dot(X)\n",
    "        return grad\n",
    "    \n",
    "    def gradientDescent (self,X,y):\n",
    "        \n",
    "        '''function which runs the gradient descent algorithm for logistic regression'''\n",
    "        \n",
    "        costSequence=[]\n",
    "        lastCost=float('inf')\n",
    "        \n",
    "        for i in tqdm(range(self.maxIteration)):\n",
    "            \n",
    "            self.w=self.w-self.learningRate*self.gradient(X,y)\n",
    "            \n",
    "            currentCost = self.costFunction(X,y)\n",
    "            diff = lastCost-currentCost\n",
    "            \n",
    "            lastCost=currentCost\n",
    "            costSequence.append(currentCost)\n",
    "            \n",
    "            if abs(diff) < self.tolerance:\n",
    "                print(\"The Model Stopped - No Further Improvement\")\n",
    "                break\n",
    "                \n",
    "        self.plotCost(costSequence)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def plotCost(self,costSequence):\n",
    "        \n",
    "        '''function to plot the cost of the model'''\n",
    "        \n",
    "        s=np.array(costSequence)\n",
    "        t=np.arange(s.size)\n",
    "        \n",
    "        fig,ax=plt.subplots()\n",
    "        ax.plot(t,s)\n",
    "        \n",
    "        ax.set(xlabel='iterations',ylabel='cost',title='cost trend')\n",
    "        ax.grid()\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05,1),shadow=True)\n",
    "        plt.show()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        '''function to return the predicted classes'''\n",
    "        \n",
    "        sig=self.sigmoid(X.dot(self.w))\n",
    "        \n",
    "        return np.around(sig)\n",
    "    \n",
    "    def evaluate(self,y,y_hat):\n",
    "        \n",
    "        '''fucntion to evaluate the performance of the model'''\n",
    "        \n",
    "        y=(y==1)\n",
    "        y_hat=(y_hat == 1)\n",
    "        \n",
    "        accuracy= ( y == y_hat).sum()/y.size\n",
    "        precision=(y & y_hat).sum() /y_hat.sum()\n",
    "        recall = (y & y_hat).sum()/y.sum()\n",
    "        f1score=2*(precision*recall)/(precision+recall)\n",
    "        \n",
    "        return accuracy,precision,recall,f1score\n",
    "    \n",
    "    def runModel(self):\n",
    "        \n",
    "        '''function to run the model'''\n",
    "        \n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.y,stratify=self.y,test_size=0.2,random_state=0)\n",
    "        self.X_train,self.mean,self.std=self.normalize(self.X_train)\n",
    "        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)\n",
    "        \n",
    "        self.w = np.ones(self.X_train.shape[1],dtype=np.float64)*0\n",
    "        self.gradientDescent(self.X_train,self.y_train)\n",
    "        \n",
    "       \n",
    "        \n",
    "        y_hat_train = self.predict(self.X_train)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_train,y_hat_train)\n",
    "        print('This is for y_train')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        y_hat_test = self.predict(self.X_test)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_test,y_hat_test)\n",
    "        print('This is for y_test')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                        | 22842/50000 [02:23<02:50, 158.90it/s]\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Stopped - No Further Improvement\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for y_train\n",
      "Training Accuracy: 0.8923910639239107\n",
      "Training Precision: 0.6317016317016317\n",
      "Training Recall: 0.19215315528243915\n",
      "Training f1score: 0.2946719826023922\n",
      "This is for y_test\n",
      "Training Accuracy: 0.8914077186774301\n",
      "Training Precision: 0.6217948717948718\n",
      "Training Recall: 0.1833648393194707\n",
      "Training f1score: 0.2832116788321168\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression(marketing_X,marketing_y,tolerance=0.0001,learningRate=0.000001)\n",
    "lr.runModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_withSMOTE:\n",
    "    \n",
    "    \"\"\"Class that performs Logistic Regression with SMOTE, addressing the class imbalance problem:\n",
    "    ---------------\n",
    "    Parameters:\n",
    "    X-features\n",
    "    y-target variable\n",
    "    learningRate-0.000001\n",
    "    tolerance-0.0001\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,X,y, learningRate,tolerance, maxIteration=50000):\n",
    "        \n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.learningRate=learningRate\n",
    "        self.tolerance=tolerance\n",
    "        self.maxIteration=maxIteration\n",
    "        \n",
    "    def normalize(self,X):\n",
    "        \n",
    "        '''function to scale the train data'''\n",
    "        \n",
    "        mean=np.mean(X)\n",
    "        std=np.std(X)\n",
    "        X_norm=(X-mean)/std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm,mean,std\n",
    "    \n",
    "    def normalizeTestData(self,X_test,train_mean,train_std):\n",
    "        \n",
    "        '''function to scale the test data'''\n",
    "        \n",
    "        X_norm=(X_test-train_mean)/train_std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm\n",
    "        \n",
    "    \n",
    "    def addX0(self,X):\n",
    "        \n",
    "        '''function to add bias to the dataset'''\n",
    "        \n",
    "        return np.column_stack([np.ones([X.shape[0],1]),X])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        \n",
    "        '''function for calculating the probability of belonging to a particular class'''\n",
    "        \n",
    "        sig=1/(1+np.exp(-z))\n",
    "        return sig\n",
    "    \n",
    "    def costFunction(self,X,y):\n",
    "        \n",
    "        '''function for returning the cost of the model'''\n",
    "        \n",
    "#         #approach 1\n",
    "        \n",
    "        pred_=np.log(np.ones(X.shape[0])+np.exp(X.dot(self.w))) - X.dot(self.w).dot(y) #negative log-likelihood\n",
    "        cost=pred_.sum()\n",
    "        \n",
    "        #approach 2\n",
    "#         sig=self.sigmoid(X.dot(self.w))\n",
    "#         pred_= y * np.log(sig) + (1-y) * np.log(1-sig)\n",
    "#         cost= pred_.sum()\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def gradient(self,X,y):\n",
    "        \n",
    "         '''function for calculationg the gradient'''\n",
    "        \n",
    "        sig=self.sigmoid(X.dot(self.w))\n",
    "        grad=(sig-y).dot(X)\n",
    "        return grad\n",
    "    \n",
    "    def gradientDescent (self,X,y):\n",
    "        \n",
    "        '''function which runs the gradient descent algorithm for logistic regression'''\n",
    "        \n",
    "        costSequence=[]\n",
    "        lastCost=float('inf')\n",
    "        \n",
    "        for i in tqdm(range(self.maxIteration)):\n",
    "            \n",
    "            self.w=self.w-self.learningRate*self.gradient(X,y)\n",
    "            \n",
    "            currentCost = self.costFunction(X,y)\n",
    "            diff = lastCost-currentCost\n",
    "            \n",
    "            lastCost=currentCost\n",
    "            costSequence.append(currentCost)\n",
    "            \n",
    "            if abs(diff) < self.tolerance:\n",
    "                print(\"The Model Stopped - No Further Improvement\")\n",
    "                break\n",
    "                \n",
    "        self.plotCost(costSequence)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def plotCost(self,costSequence):\n",
    "        \n",
    "        '''function to plot the cost of the model'''\n",
    "        \n",
    "        s=np.array(costSequence)\n",
    "        t=np.arange(s.size)\n",
    "        \n",
    "        fig,ax=plt.subplots()\n",
    "        ax.plot(t,s)\n",
    "        \n",
    "        ax.set(xlabel='iterations',ylabel='cost',title='cost trend')\n",
    "        ax.grid()\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05,1),shadow=True)\n",
    "        plt.show()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        '''function to return the predicted classes'''\n",
    "        \n",
    "        sig=self.sigmoid(X.dot(self.w))\n",
    "        \n",
    "        return np.around(sig)\n",
    "    \n",
    "    def evaluate(self,y,y_hat):\n",
    "        \n",
    "        '''fucntion to evaluate the performance of the model'''\n",
    "        \n",
    "        y=(y==1)\n",
    "        y_hat=(y_hat == 1)\n",
    "        \n",
    "        accuracy= ( y == y_hat).sum()/y.size\n",
    "        precision=(y & y_hat).sum() /y_hat.sum()\n",
    "        recall = (y & y_hat).sum()/y.sum()\n",
    "        f1score=2*(precision*recall)/(precision+recall)\n",
    "        \n",
    "        return accuracy,precision,recall,f1score\n",
    "    \n",
    "    def runModel(self):\n",
    "        \n",
    "        '''function to run the model'''\n",
    "        \n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.y,stratify=self.y,test_size=0.2,random_state=0)\n",
    "        \n",
    "        #normalizing the data\n",
    "        self.X_train,self.mean,self.std=self.normalize(self.X_train)\n",
    "        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)\n",
    "        \n",
    "        #solving class imbalance using SMOTE\n",
    "        smote = SMOTETomek(random_state=101)\n",
    "        self.X_train,self.y_train = smote.fit_resample(self.X_train,self.y_train)\n",
    "        \n",
    "        self.w = np.ones(self.X_train.shape[1],dtype=np.float64)*0\n",
    "        self.gradientDescent(self.X_train,self.y_train)\n",
    "        \n",
    "       \n",
    "        \n",
    "        y_hat_train = self.predict(self.X_train)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_train,y_hat_train)\n",
    "        print('This is for y_train')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        y_hat_test = self.predict(self.X_test)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_test,y_hat_test)\n",
    "        print('This is for y_test')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [13:12<00:00, 63.09it/s]\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for y_train\n",
      "Training Accuracy: 0.7161842229868367\n",
      "Training Precision: 0.7446154945172697\n",
      "Training Recall: 0.6580700148363269\n",
      "Training f1score: 0.698672833299819\n",
      "This is for y_test\n",
      "Training Accuracy: 0.7601459692579896\n",
      "Training Precision: 0.2760983474405482\n",
      "Training Recall: 0.6474480151228733\n",
      "Training f1score: 0.38711500423848544\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression_withSMOTE(marketing_X,marketing_y,tolerance=0.0001,learningRate=0.000001)\n",
    "lr.runModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMarginSVM:\n",
    "    \n",
    "    \"\"\"Class that performs Soft margin SVM:\n",
    "    ---------------\n",
    "    Parameters:\n",
    "    X-features\n",
    "    y-target variable\n",
    "    alpha-lagrangian multiplier\n",
    "    w-weights(parameters which are learnt)\n",
    "    C-regularization term\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,X,y,C):\n",
    "        self.alpha = None\n",
    "        self.w = None\n",
    "        self.supportVectors = None\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.C = C\n",
    "        \n",
    "    def normalize(self,X):\n",
    "        \n",
    "        '''function to scale the train data'''\n",
    "        \n",
    "        mean=np.mean(X)\n",
    "        std=np.std(X)\n",
    "        X_norm=(X-mean)/std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm,mean,std\n",
    "    \n",
    "    def normalizeTestData(self,X_test,train_mean,train_std):\n",
    "        \n",
    "        '''function to scale the test data'''\n",
    "        \n",
    "        X_norm=(X_test-train_mean)/train_std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm\n",
    "        \n",
    "    \n",
    "    def addX0(self,X):\n",
    "        \n",
    "        ''' function to add bias to the dataset'''\n",
    "        \n",
    "        return np.column_stack([np.ones([X.shape[0],1]),X])\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''function for fitting the model'''\n",
    "\n",
    "        N = len(y)\n",
    "\n",
    "        # Gram matrix of (X,y)\n",
    "        Xy = X*y[:,np.newaxis] \n",
    "\n",
    "        GramXy = np.matmul(Xy,Xy.T) \n",
    "    \n",
    "        def Ld0(G, alpha): \n",
    "            \n",
    "            '''function to calculate the dual form of SVM'''\n",
    "            obj_fn = alpha.sum() - 0.5*alpha.dot(alpha.dot(G))\n",
    "\n",
    "            return obj_fn\n",
    "        \n",
    "\n",
    "            # Derivative of Lagrangian Function\n",
    "        def partialDerivationLd0(G, alpha):\n",
    "            \n",
    "            '''function to calculate derivative of lagrangian function'''\n",
    "            \n",
    "            par_der = np.ones_like(alpha) - alpha.dot(G)\n",
    "            return par_der\n",
    "\n",
    "        alpha = np.ones(N)\n",
    "\n",
    "        A = np.vstack((-np.eye(N), np.eye(N)))\n",
    "        b = np.concatenate((np.zeros(N), self.C * np.ones(N)))\n",
    "\n",
    "        constraints = ({'type': 'eq', 'fun':lambda a: np.dot(a,y), 'jac': lambda a: y },\n",
    "                    {'type': 'ineq', 'fun':lambda a: b - np.dot(A, a), 'jac': lambda a: -A}) \n",
    "\n",
    "        optRes = optimize.minimize(fun = lambda a: -Ld0(GramXy, a),\n",
    "                                x0 = alpha,\n",
    "                                method = 'SLSQP',\n",
    "                                jac = lambda a: - partialDerivationLd0(GramXy, a),\n",
    "                                constraints = constraints)\n",
    "        \n",
    "        self.alpha = optRes.x\n",
    "\n",
    "\n",
    "        self.w =  np.sum(( self.alpha[:, np.newaxis] * Xy), axis = 0)\n",
    "\n",
    "        epsilon = 1e-4\n",
    "        self.supportVectors = X[ self.alpha > epsilon]\n",
    "        self.supportLabels = y[self.alpha > epsilon]\n",
    "\n",
    "\n",
    "        self.b1 = self.supportLabels[0] - np.matmul(self.supportVectors[0].T, self.w)\n",
    "        \n",
    "        b = []\n",
    "        for i in tqdm(range(len(self.supportLabels))):\n",
    "            b_i = self.supportLabels[i] - np.matmul(self.supportVectors[i].T, self.w)\n",
    "            b.append( b_i )\n",
    "        \n",
    "        self.b2 = sum(b)/len(b)\n",
    "        \n",
    "        self.intercept = self.b2\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        '''function to return the predicted classes'''\n",
    "    \n",
    "        return 2*(np.matmul(X, self.w) + self.intercept > 0) - 1\n",
    "    \n",
    "    def evaluate(self,y,y_hat):\n",
    "        \n",
    "        '''function to evaluate the performance of the model'''\n",
    "        \n",
    "        y=(y==1)\n",
    "        y_hat=(y_hat == 1)\n",
    "        \n",
    "        accuracy= ( y == y_hat).sum()/y.size\n",
    "        precision=(y & y_hat).sum() /y_hat.sum()\n",
    "        recall = (y & y_hat).sum()/y.sum()\n",
    "        f1score=2*(precision*recall)/(precision+recall)\n",
    "        \n",
    "        return accuracy,precision,recall,f1score\n",
    "    \n",
    "    def runModel(self):\n",
    "        \n",
    "        '''function to run the model'''\n",
    "        \n",
    "        #solving class imbalance using SMOTE\n",
    "        \n",
    "        smote=SMOTETomek(random_state=42)\n",
    "        self.X,self.y= smote.fit_resample(self.X,self.y)\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.y,test_size=0.2,random_state=0)\n",
    "        \n",
    "        #normalizing the data\n",
    "        self.X_train,self.mean,self.std=self.normalize(self.X_train)\n",
    "        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)\n",
    "        \n",
    "        self.fit(self.X_train,self.y_train)\n",
    "        \n",
    "        y_hat_train = self.predict(self.X_train)\n",
    "        print('This is y_hat_train:', y_hat_train)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_train,y_hat_train)\n",
    "        print('This is for y_train')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        y_hat_test = self.predict(self.X_test)\n",
    "        print('This is y_hat_test:', y_hat_test)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_test,y_hat_test)\n",
    "        print('This is for y_test')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftMarginSVM(marketing_X.values[:2500, :],marketing_y.values[:2500],C=1)\n",
    "model.runModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMarginSVM_kernel:\n",
    "    \n",
    "    \n",
    "    \"\"\"Class that performs Soft margin SVM:\n",
    "    ---------------\n",
    "    Parameters:\n",
    "    X-features\n",
    "    y-target variable\n",
    "    alpha-lagrangian multiplier\n",
    "    w-weights(parameters which are learnt)\n",
    "    C-regularization term\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,X,y,C):\n",
    "        self.alpha = None\n",
    "        self.w = None\n",
    "        self.supportVectors = None\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.C = C\n",
    "    def kernel_helper(self, xa, xb):\n",
    "        \n",
    "        '''function to calculate the RBF kernel'''\n",
    "        p=(xa-xb)\n",
    "        pa=p.T.dot(p)\n",
    "        return int(pa)\n",
    "        \n",
    "    def Kernel(self,X):\n",
    "        \n",
    "        '''function to calculate the RBF kernel'''\n",
    "        oper=0\n",
    "        std=0.1\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(X)):\n",
    "                xi=X[i]\n",
    "                xj=X[j]\n",
    "                oper+=self.kernel_helper(xi,xj)\n",
    "        oper=oper/2*std*std\n",
    "        return np.exp(-oper)\n",
    "    \n",
    "    def normalize(self,X):\n",
    "        \n",
    "        '''function to scale the train data'''\n",
    "        \n",
    "        mean=np.mean(X)\n",
    "        std=np.std(X)\n",
    "        X_norm=(X-mean)/std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm,mean,std\n",
    "    \n",
    "    def normalizeTestData(self,X_test,train_mean,train_std):\n",
    "        \n",
    "        '''function to scale the train data'''\n",
    "        \n",
    "        X_norm=(X_test-train_mean)/train_std\n",
    "        X_norm=self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm\n",
    "        \n",
    "    \n",
    "    def addX0(self,X):\n",
    "        \n",
    "        '''function to add bias to the dataset'''\n",
    "        \n",
    "        return np.column_stack([np.ones([X.shape[0],1]),X])\n",
    "                \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''function to fit the model'''\n",
    "\n",
    "        N = len(y)\n",
    "\n",
    "#         # Gram matrix of (X,y)\n",
    "        Xy = X*y[:,np.newaxis] \n",
    "\n",
    "        GramXy = y.T.dot(y)\n",
    "        GramXy*=self.Kernel(X)\n",
    "    \n",
    "        def Ld0(G, alpha):\n",
    "            \n",
    "            '''function to calculate the dual form of SVM'''\n",
    "            obj_fn = alpha.sum() - 0.5*alpha.dot(alpha.dot(G))\n",
    "\n",
    "            return obj_fn\n",
    "        \n",
    "\n",
    "            # Derivative of Lagrangian Function\n",
    "        def partialDerivationLd0(G, alpha):\n",
    "            \n",
    "            '''function to calculate derivative of lagrangian function'''\n",
    "            par_der = np.ones_like(alpha) - alpha.dot(G)\n",
    "            return par_der\n",
    "\n",
    "        alpha = np.ones(N)\n",
    "\n",
    "        A = np.vstack((-np.eye(N), np.eye(N)))\n",
    "        b = np.concatenate((np.zeros(N), self.C * np.ones(N)))\n",
    "\n",
    "        constraints = ({'type': 'eq', 'fun':lambda a: np.dot(a,y), 'jac': lambda a: y },\n",
    "                    {'type': 'ineq', 'fun':lambda a: b - np.dot(A, a), 'jac': lambda a: -A}) \n",
    "\n",
    "        optRes = optimize.minimize(fun = lambda a: -Ld0(GramXy, a),\n",
    "                                x0 = alpha,\n",
    "                                method = 'SLSQP',\n",
    "                                jac = lambda a: - partialDerivationLd0(GramXy, a),\n",
    "                                constraints = constraints)\n",
    "        \n",
    "        self.alpha = optRes.x\n",
    "\n",
    "\n",
    "        self.w =  np.sum(( self.alpha[:, np.newaxis] * Xy), axis = 0)\n",
    "\n",
    "        epsilon = 1e-4\n",
    "        self.supportVectors = X[ self.alpha > epsilon]\n",
    "        self.supportLabels = y[self.alpha > epsilon]\n",
    "\n",
    "\n",
    "        self.b1 = self.supportLabels[0] - np.matmul(self.supportVectors[0].T, self.w)\n",
    "        \n",
    "        b = []\n",
    "        for i in tqdm(range(len(self.supportLabels))):\n",
    "            b_i = self.supportLabels[i] - np.matmul(self.supportVectors[i].T, self.w)\n",
    "            b.append( b_i )\n",
    "        \n",
    "        self.b2 = sum(b)/len(b)\n",
    "        \n",
    "        self.intercept = self.b2\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        '''function to return the predicted classes'''\n",
    "    \n",
    "        return 2*(np.matmul(X, self.w) + self.intercept > 0) - 1\n",
    "    \n",
    "    def evaluate(self,y,y_hat):\n",
    "        \n",
    "        '''function to evaluate the performance of the model'''\n",
    "        \n",
    "        y=(y==1)\n",
    "        y_hat=(y_hat == 1)\n",
    "        \n",
    "        accuracy= ( y == y_hat).sum()/y.size\n",
    "        precision=(y & y_hat).sum() /y_hat.sum()\n",
    "        recall = (y & y_hat).sum()/y.sum()\n",
    "        f1score=2*(precision*recall)/(precision+recall)\n",
    "        \n",
    "        return accuracy,precision,recall,f1score\n",
    "    \n",
    "    def runModel(self):\n",
    "        \n",
    "        '''function to run the model'''\n",
    "        \n",
    "        \n",
    "        #solving class imbalance using SMOTE\n",
    "        \n",
    "        smote=SMOTETomek(random_state=42)\n",
    "        self.X,self.y= smote.fit_resample(self.X,self.y)\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.y,test_size=0.2,random_state=0)\n",
    "        \n",
    "        #normalizing the data\n",
    "        self.X_train,self.mean,self.std=self.normalize(self.X_train)\n",
    "        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)\n",
    "        \n",
    "        self.fit(self.X_train,self.y_train)\n",
    "        \n",
    "        y_hat_train = self.predict(self.X_train)\n",
    "        print('This is y_hat_train:', y_hat_train)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_train,y_hat_train)\n",
    "        print('This is for y_train')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        y_hat_test = self.predict(self.X_test)\n",
    "        print('This is y_hat_test:', y_hat_test)\n",
    "        accuracy,precision,recall,f1score=self.evaluate(self.y_test,y_hat_test)\n",
    "        print('This is for y_test')\n",
    "        print('Training Accuracy:', accuracy)\n",
    "        print('Training Precision:', precision)\n",
    "        print('Training Recall:',recall)\n",
    "        print('Training f1score:',f1score)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1957/1957 [00:00<00:00, 191633.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is y_hat_train: [ 1 -1  1 ...  1 -1 -1]\n",
      "This is for y_train\n",
      "Training Accuracy: 0.4962868117797695\n",
      "Training Precision: 0.49501835343471423\n",
      "Training Recall: 0.48459958932238195\n",
      "Training f1score: 0.4897535667963684\n",
      "This is y_hat_test: [-1 -1  1  1  1  1 -1  1 -1 -1  1 -1 -1 -1  1 -1  1  1  1 -1  1  1  1  1\n",
      "  1 -1 -1 -1  1  1 -1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1 -1  1  1 -1  1 -1  1 -1  1 -1 -1  1  1 -1  1 -1  1 -1 -1  1 -1\n",
      " -1 -1  1 -1 -1 -1  1  1  1  1  1  1 -1  1  1  1  1 -1 -1  1 -1  1 -1  1\n",
      "  1 -1 -1 -1 -1 -1  1  1  1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1\n",
      "  1  1 -1 -1  1 -1 -1  1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1 -1 -1  1\n",
      " -1 -1  1 -1 -1 -1 -1  1  1 -1 -1  1 -1 -1 -1  1  1 -1  1 -1 -1  1  1 -1\n",
      " -1  1  1  1  1  1  1  1 -1 -1  1  1  1 -1  1  1 -1 -1 -1  1 -1  1  1 -1\n",
      " -1  1 -1 -1  1  1 -1 -1 -1  1 -1  1 -1 -1 -1  1  1  1 -1  1  1 -1  1 -1\n",
      " -1 -1 -1 -1  1  1  1  1  1 -1  1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1\n",
      "  1  1 -1  1 -1  1 -1 -1  1  1 -1 -1  1  1  1  1  1  1  1 -1  1 -1 -1 -1\n",
      " -1 -1  1  1 -1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1 -1 -1 -1  1  1 -1 -1  1 -1 -1 -1  1 -1 -1 -1\n",
      "  1  1  1 -1  1 -1 -1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1  1 -1  1  1 -1\n",
      "  1  1 -1  1 -1 -1  1 -1  1 -1  1 -1 -1  1  1 -1  1  1 -1  1 -1  1 -1  1\n",
      " -1 -1 -1 -1 -1 -1  1  1 -1  1  1 -1  1 -1  1 -1  1  1 -1  1 -1  1  1  1\n",
      "  1  1  1 -1  1 -1 -1 -1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1  1  1  1  1  1\n",
      "  1 -1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1  1  1 -1  1 -1  1  1 -1 -1  1  1\n",
      " -1 -1  1  1  1  1  1 -1 -1 -1  1 -1 -1  1 -1 -1 -1  1 -1 -1  1 -1  1 -1\n",
      " -1 -1  1  1 -1  1 -1  1 -1  1 -1  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1  1\n",
      " -1 -1 -1  1 -1 -1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1 -1 -1 -1  1 -1\n",
      " -1 -1  1 -1  1  1 -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1 -1  1  1  1 -1 -1  1\n",
      "  1 -1  1  1  1 -1  1  1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1 -1  1 -1 -1  1\n",
      " -1 -1  1  1 -1 -1 -1 -1  1 -1  1  1  1  1  1 -1 -1 -1  1  1  1 -1 -1 -1\n",
      "  1 -1 -1 -1  1  1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  1 -1  1  1  1  1  1\n",
      " -1  1  1 -1  1  1  1 -1  1 -1 -1  1  1  1 -1 -1  1 -1 -1  1 -1 -1  1 -1\n",
      " -1  1  1 -1 -1  1 -1 -1 -1  1 -1 -1  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1\n",
      "  1 -1 -1 -1  1 -1 -1  1  1 -1  1  1  1  1  1  1  1  1 -1 -1 -1  1  1 -1\n",
      "  1 -1  1 -1  1 -1 -1 -1 -1 -1 -1  1 -1  1  1 -1  1 -1  1  1 -1 -1 -1  1\n",
      " -1 -1  1 -1  1 -1  1  1  1  1 -1 -1  1 -1  1 -1 -1  1 -1  1  1 -1 -1 -1\n",
      " -1  1 -1  1  1 -1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1  1  1  1  1 -1  1\n",
      " -1 -1  1  1 -1 -1 -1 -1 -1  1  1  1 -1  1  1 -1 -1  1  1 -1  1  1 -1  1\n",
      " -1 -1 -1 -1 -1 -1 -1  1  1 -1  1  1 -1  1  1  1 -1 -1  1  1 -1  1 -1 -1\n",
      " -1  1  1  1 -1 -1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1\n",
      " -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1  1  1  1 -1 -1 -1  1 -1  1\n",
      "  1 -1  1 -1  1  1  1  1 -1 -1 -1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1  1  1\n",
      " -1  1  1 -1  1  1 -1  1  1 -1 -1 -1  1 -1 -1 -1  1  1  1  1 -1  1  1  1\n",
      " -1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1 -1  1  1  1\n",
      "  1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1 -1 -1  1 -1  1 -1 -1  1 -1  1 -1\n",
      "  1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1  1 -1  1  1 -1  1  1 -1 -1  1 -1 -1\n",
      " -1  1  1 -1  1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1]\n",
      "This is for y_test\n",
      "Training Accuracy: 0.4984646878198567\n",
      "Training Precision: 0.5031315240083507\n",
      "Training Recall: 0.48884381338742394\n",
      "Training f1score: 0.4958847736625514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = SoftMarginSVM_kernel(marketing_X.values[:2500, :],marketing_y.values[:2500],C=1)\n",
    "model1.runModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
